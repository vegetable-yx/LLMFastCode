Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include "common.h"

void slow_performance(double *__restrict x, double *__restrict y, double *__restrict z)
{
    for (int i = 0; i < 32; ++i) {
        z[i] = 0.0;
    }

    for (int i = 0; i < 4; ++i) {
        for (int j = 0; j < 4; ++j) {
            double real = 0.0;
            double imag = 0.0;
            for (int k = 0; k < 4; ++k) {
                double a = x[(i * 4 + k) * 2];
                double b = x[(i * 4 + k) * 2 + 1];

                double c = y[(k * 4 + j) * 2];
                double d = y[(k * 4 + j) * 2 + 1];

                double prod_real = 2.0 * a * c - b * d;
                double prod_imag = 2.0 * b * c + 2.0 * a * d;

                real += prod_real;
                imag += prod_imag;
            }

            z[(i * 4 + j) * 2] = real;
            z[(i * 4 + j) * 2 + 1] = imag;
        }
    }
}

void slow_performance(double *__restrict x, double *__restrict y, double *__restrict z)
{
}
```


================================
```cpp
#include <immintrin.h>

void max_performance(double *__restrict x, double *__restrict y, double *__restrict z)
{
    __m256d zero = _mm256_setzero_pd();
    for (int i = 0; i < 32; i += 4) {
        _mm256_store_pd(z + i, zero);
    }

    for (int i = 0; i < 4; ++i) {
        for (int j = 0; j < 4; ++j) {
            __m256d real_vec = _mm256_setzero_pd();
            __m256d imag_vec = _mm256_setzero_pd();

            for (int k = 0; k < 4; ++k) {
                int xi = (i * 4 + k) * 2;
                int yj = (k * 4 + j) * 2;

                __m128d a_b = _mm_load_pd(x + xi);
                __m128d c_d = _mm_load_pd(y + yj);

                __m128d ac = _mm_mul_pd(_mm_set1_pd(2.0), _mm_mul_pd(_mm_shuffle_pd(a_b, a_b, 0x0), _mm_shuffle_pd(c_d, c_d, 0x0)));
                __m128d bd = _mm_mul_pd(_mm_shuffle_pd(a_b, a_b, 0x3), _mm_shuffle_pd(c_d, c_d, 0x3));
                __m128d real_part = _mm_sub_sd(ac, bd);

                __m128d bc = _mm_mul_pd(_mm_shuffle_pd(a_b, a_b, 0x3), _mm_shuffle_pd(c_d, c_d, 0x0));
                __m128d ad = _mm_mul_pd(_mm_shuffle_pd(a_b, a_b, 0x0), _mm_shuffle_pd(c_d, c_d, 0x3));
                __m128d imag_part = _mm_add_sd(bc, ad);
                imag_part = _mm_mul_sd(_mm_set1_pd(2.0), imag_part);

                real_vec = _mm256_insertf128_pd(real_vec, _mm_add_sd(_mm256_castpd256_pd128(real_vec), real_part), 0);
                imag_vec = _mm256_insertf128_pd(imag_vec, _mm_add_sd(_mm256_castpd256_pd128(imag_vec), imag_part), 0);
            }

            int zi = (i * 4 + j) * 2;
            _mm_store_sd(z + zi, _mm256_castpd256_pd128(real_vec));
            _mm_store_sd(z + zi + 1, _mm256_castpd256_pd128(imag_vec));
        }
    }
}
```
>>>
Starting program. 2 functions registered.

Running: slow_performance
56.1502 cycles

Running: maxperformance
181.387 cycles
<<<
56.1502 / 181.387 = 0.30956022206663103
================================
```
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict y, double *__restrict z)
{
    __m256d zero = _mm256_setzero_pd();
    for (int i = 0; i < 32; i += 4) {
        _mm256_store_pd(z + i * 2, zero);
    }

    for (int i = 0; i < 4; ++i) {
        for (int j = 0; j < 4; ++j) {
            __m256d real_acc = _mm256_setzero_pd();
            __m256d imag_acc = _mm256_setzero_pd();
            for (int k = 0; k < 4; ++k) {
                int idx_x = (i * 4 + k) * 2;
                int idx_y = (k * 4 + j) * 2;

                __m128d a_b = _mm_load_pd(x + idx_x);
                __m128d c_d = _mm_load_pd(y + idx_y);

                __m128d ac = _mm_mul_pd(a_b, _mm_unpacklo_pd(c_d, c_d));
                __m128d ad = _mm_mul_pd(a_b, _mm_unpackhi_pd(c_d, c_d));

                __m128d ac_scaled = _mm_mul_pd(ac, _mm_set_pd(1.0, 2.0));

                __m128d prod_real = _mm_sub_sd(_mm_unpackhi_pd(ac_scaled, ac_scaled), _mm_unpacklo_pd(ad, ad));
                __m128d prod_imag = _mm_add_sd(_mm_unpackhi_pd(ad, ad), _mm_unpacklo_pd(ad, ad));
                prod_imag = _mm_add_sd(prod_imag, _mm_unpacklo_pd(ac_scaled, ac_scaled));

                real_acc = _mm256_add_pd(real_acc, _mm256_castpd128_pd256(prod_real));
                imag_acc = _mm256_add_pd(imag_acc, _mm256_castpd128_pd256(prod_imag));
            }
            __m128d real_sum = _mm_add_pd(_mm256_castpd256_pd128(real_acc), _mm256_extractf128_pd(real_acc, 1));
            __m128d imag_sum = _mm_add_pd(_mm256_castpd256_pd128(imag_acc), _mm256_extractf128_pd(imag_acc, 1));
            double real_final = _mm_cvtsd_f64(real_sum);
            double imag_final = _mm_cvtsd_f64(imag_sum);
            int idx_z = (i * 4 + j) * 2;
            z[idx_z] = real_final;
            z[idx_z + 1] = imag_final;
        }
    }
}
```
>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance
55.0035 cycles
Segmentation fault
<<<

================================
```cpp
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict y, double *__restrict z)
{
    __m256d zero = _mm256_setzero_pd();
    for (int i = 0; i < 32; i += 4) {
        _mm256_store_pd(&z[i], zero);
    }

    for (int i = 0; i < 4; ++i) {
        for (int j = 0; j < 4; ++j) {
            __m256d real_sum = _mm256_setzero_pd();
            __m256d imag_sum = _mm256_setzero_pd();

            for (int k = 0; k < 4; ++k) {
                int x_idx = (i * 4 + k) * 2;
                int y_idx = (k * 4 + j) * 2;

                __m128d a_b = _mm_loadu_pd(&x[x_idx]);
                __m128d c_d = _mm_loadu_pd(&y[y_idx]);

                __m128d a = _mm_permute_pd(a_b, 0x0); // [a, a]
                __m128d b = _mm_permute_pd(a_b, 0x3); // [b, b]
                __m128d c = _mm_permute_pd(c_d, 0x0); // [c, c]
                __m128d d = _mm_permute_pd(c_d, 0x3); // [d, d]

                __m128d ac2 = _mm_mul_pd(_mm_add_pd(a, a), c);     // 2a*c
                __m128d bd  = _mm_mul_pd(b, d);                    // b*d
                __m128d real = _mm_sub_pd(ac2, bd);                // real = 2a*c - b*d

                __m128d bc2 = _mm_mul_pd(_mm_add_pd(b, b), c);     // 2b*c
                __m128d ad2 = _mm_mul_pd(_mm_add_pd(a, a), d);     // 2a*d
                __m128d imag = _mm_add_pd(bc2, ad2);               // imag = 2b*c + 2a*d

                __m256d real_imag = _mm256_insertf128_pd(_mm256_castpd128_pd256(real), imag, 1);
                real_sum = _mm256_add_pd(real_sum, _mm256_permute2f128_pd(real_imag, real_imag, 0x00));
                imag_sum = _mm256_add_pd(imag_sum, _mm256_permute2f128_pd(real_imag, real_imag, 0x11));
            }

            double real_array[4], imag_array[4];
            _mm256_storeu_pd(real_array, real_sum);
            _mm256_storeu_pd(imag_array, imag_sum);
            z[(i * 4 + j) * 2]     = real_array[0];
            z[(i * 4 + j) * 2 + 1] = imag_array[0];
        }
    }
}
```
>>>
Starting program. 2 functions registered.

Running: slow_performance
55.1398 cycles

Running: maxperformance
188.872 cycles
<<<
55.1398 / 188.872 = 0.2919426913465204
================================
