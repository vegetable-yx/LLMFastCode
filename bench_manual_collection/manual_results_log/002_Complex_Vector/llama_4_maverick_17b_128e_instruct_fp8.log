Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include "common.h"

// Precondition: 0 <= z[i] < 1.0
void slow_performance(double *__restrict x, double *__restrict y, double *__restrict z)
{
  double t[2];
  for (int i = 0; i < N; i++)
  {
    if (sqrt(x[i * 2] * x[i * 2] + x[i * 2 + 1] * x[i * 2 + 1]) > sqrt(y[i * 2] * y[i * 2] + y[i * 2 + 1] * y[i * 2 + 1]))
    {
      unsigned int k = floor(4.0 * z[i]);
      switch (k)
      {
      case 0:
        t[0] = x[i * 2] + y[i * 2] * 5.1;
        t[1] = x[i * 2 + 1] + y[i * 2 + 1] * 5.1;
        break;
      case 1:
        t[0] = (x[i * 2]) * (y[i * 2] * 2.1) - (x[i * 2 + 1]) * (y[i * 2 + 1] * 2.1);
        t[1] = (x[i * 2]) * (y[i * 2 + 1] * 2.1) + (x[i * 2 + 1]) * (y[i * 2] * 2.1);
        break;
      case 2:
        t[0] = x[i * 2] + y[i * 2] * (-3.5);
        t[1] = x[i * 2 + 1] + y[i * 2 + 1] * (-3.5);
        break;
      case 3:
        t[0] = (x[i * 2]) * (y[i * 2] * (-4.1)) - (x[i * 2 + 1]) * (y[i * 2 + 1] * (-4.1));
        t[1] = (x[i * 2]) * (y[i * 2 + 1] * (-4.1)) + (x[i * 2 + 1]) * (y[i * 2] * (-4.1));
        break;
      default:
        break;
      }
      z[i] += t[0] + ((t[0]) * (t[0]) - (t[1]) * (t[1]));
    }
    else
    {
      z[i] += round(((x[i * 2]) * (x[i * 2]) - (x[i * 2 + 1]) * (x[i * 2 + 1]))) + pow(sqrt(y[i * 2] * y[i * 2] + y[i * 2 + 1] * y[i * 2 + 1]), 2.0);
    }
  }
}

void maxperformance(double *__restrict x, double *__restrict y, double *__restrict z)
{
}
```


================================
#include <immintrin.h>

void max_performance(double *__restrict x, double *__restrict y, double *__restrict z) {
  const __m256d five_one = _mm256_set1_pd(5.1);
  const __m256d two_one = _mm256_set1_pd(2.1);
  const __m256d minus_three_five = _mm256_set1_pd(-3.5);
  const __m256d minus_four_one = _mm256_set1_pd(-4.1);
  const __m256d four = _mm256_set1_pd(4.0);

  for (int i = 0; i < N; i++) {
    __m256d x_vec = _mm256_loadu_pd(&x[i * 2]);
    __m256d y_vec = _mm256_loadu_pd(&y[i * 2]);
    __m256d x_square = _mm256_mul_pd(x_vec, x_vec);
    __m256d y_square = _mm256_mul_pd(y_vec, y_vec);
    __m256d sum_square_x = _mm256_hadd_pd(x_square, x_square);
    __m256d sum_square_y = _mm256_hadd_pd(y_square, y_square);
    __m256d sqrt_sum_square_x = _mm256_sqrt_pd(sum_square_x);
    __m256d sqrt_sum_square_y = _mm256_sqrt_pd(sum_square_y);
    __mmask8 cmp = _mm256_cmp_pd_mask(sqrt_sum_square_x, sqrt_sum_square_y, _CMP_GT_OQ);

    if (cmp & 1) {
      unsigned int k = (unsigned int)(z[i] * 4.0);
      double t0, t1;
      switch (k) {
        case 0: {
          __m256d t_vec = _mm256_fmadd_pd(y_vec, five_one, x_vec);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        case 1: {
          __m256d xy_mul = _mm256_mul_pd(x_vec, y_vec);
          __m256d yx_mul = _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), _mm256_shuffle_pd(y_vec, y_vec, 0x5));
          __m256d t_vec = _mm256_fmsub_pd(x_vec, _mm256_shuffle_pd(y_vec, y_vec, 0x5), _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), y_vec));
          t_vec = _mm256_mul_pd(t_vec, two_one);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        case 2: {
          __m256d t_vec = _mm256_fmadd_pd(y_vec, minus_three_five, x_vec);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        case 3: {
          __m256d xy_mul = _mm256_mul_pd(x_vec, y_vec);
          __m256d yx_mul = _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), _mm256_shuffle_pd(y_vec, y_vec, 0x5));
          __m256d t_vec = _mm256_fmsub_pd(x_vec, _mm256_shuffle_pd(y_vec, y_vec, 0x5), _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), y_vec));
          t_vec = _mm256_mul_pd(t_vec, minus_four_one);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        default:
          t0 = t1 = 0.0;
          break;
      }
      z[i] += t0 + (t0 * t0 - t1 * t1);
    } else {
      __m256d xx = _mm256_mul_pd(x_vec, x_vec);
      __m256d yy = _mm256_mul_pd(y_vec, y_vec);
      double x_square_sum = _mm256_cvtsd_f64(_mm256_hadd_pd(xx, xx));
      double y_square_sum = _mm256_cvtsd_f64(_mm256_hadd_pd(yy, yy));
      double y_norm_square = y_square_sum;
      z[i] += round(x_square_sum - 2 * *((double*)&x_square_sum + 1)) + y_norm_square;
    }
  }
}

>>>
make: *** [Makefile:13: benchmark] Error 1
<<<
CE
================================
#include <immintrin.h>

void max_performance(double *__restrict x, double *__restrict y, double *__restrict z) {
  const __m256d five_one = _mm256_set1_pd(5.1);
  const __m256d two_one = _mm256_set1_pd(2.1);
  const __m256d minus_three_five = _mm256_set1_pd(-3.5);
  const __m256d minus_four_one = _mm256_set1_pd(-4.1);
  const __m256d four = _mm256_set1_pd(4.0);

  for (int i = 0; i < N; i++) {
    __m256d x_vec = _mm256_loadu_pd(&x[i * 2]);
    __m256d y_vec = _mm256_loadu_pd(&y[i * 2]);
    __m256d x_square = _mm256_mul_pd(x_vec, x_vec);
    __m256d y_square = _mm256_mul_pd(y_vec, y_vec);
    __m256d sum_square_x = _mm256_hadd_pd(x_square, x_square);
    __m256d sum_square_y = _mm256_hadd_pd(y_square, y_square);
    __m256d sqrt_sum_square_x = _mm256_sqrt_pd(sum_square_x);
    __m256d sqrt_sum_square_y = _mm256_sqrt_pd(sum_square_y);
    __mmask8 cmp = _mm256_cmp_pd_mask(sqrt_sum_square_x, sqrt_sum_square_y, _CMP_GT_OQ);

    if (cmp & 1) {
      unsigned int k = (unsigned int)(z[i] * 4.0);
      double t0, t1;
      switch (k) {
        case 0: {
          __m256d t_vec = _mm256_fmadd_pd(y_vec, five_one, x_vec);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        case 1: {
          __m256d xy_mul = _mm256_mul_pd(x_vec, y_vec);
          __m256d yx_mul = _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), _mm256_shuffle_pd(y_vec, y_vec, 0x5));
          __m256d t_vec = _mm256_fmsub_pd(x_vec, _mm256_shuffle_pd(y_vec, y_vec, 0x5), _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), y_vec));
          t_vec = _mm256_mul_pd(t_vec, two_one);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        case 2: {
          __m256d t_vec = _mm256_fmadd_pd(y_vec, minus_three_five, x_vec);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        case 3: {
          __m256d xy_mul = _mm256_mul_pd(x_vec, y_vec);
          __m256d yx_mul = _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), _mm256_shuffle_pd(y_vec, y_vec, 0x5));
          __m256d t_vec = _mm256_fmsub_pd(x_vec, _mm256_shuffle_pd(y_vec, y_vec, 0x5), _mm256_mul_pd(_mm256_shuffle_pd(x_vec, x_vec, 0x5), y_vec));
          t_vec = _mm256_mul_pd(t_vec, minus_four_one);
          _mm256_storeu_pd(&t0, t_vec);
          t0 = t0; t1 = *((double*)&t0 + 1);
          break;
        }
        default:
          t0 = t1 = 0.0;
          break;
      }
      z[i] += t0 + (t0 * t0 - t1 * t1);
    } else {
      __m256d xx = _mm256_mul_pd(x_vec, x_vec);
      __m256d yy = _mm256_mul_pd(y_vec, y_vec);
      __m256d sum_square = _mm256_hadd_pd(xx, xx);
      __m256d diff_square = _mm256_hsub_pd(xx, xx);
      __m256d round_val = _mm256_round_pd(sum_square, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
      __m256d pow_val = sum_square_y;
      _mm256_storeu_pd(&x_vec, _mm256_add_pd(round_val, pow_val));
      z[i] += x_vec[0];
    }
  }
}

>>>
make: *** [Makefile:13: benchmark] Error 1
<<<
CE
================================
#include <immintrin.h>

void max_performance(double *__restrict x, double *__restrict y, double *__restrict z) {
  const __m256d cst_5_1 = _mm256_set1_pd(5.1);
  const __m256d cst_2_1 = _mm256_set1_pd(2.1);
  const __m256d cst_n3_5 = _mm256_set1_pd(-3.5);
  const __m256d cst_n4_1 = _mm256_set1_pd(-4.1);
  const __m256d cst_4 = _mm256_set1_pd(4.0);
  const __m256d cst_0 = _mm256_set1_pd(0.0);

  for (int i = 0; i < N; i++) {
    __m256d vx = _mm256_loadu_pd(&x[i * 2]);
    __m256d vy = _mm256_loadu_pd(&y[i * 2]);
    __m256d vxy_norm = _mm256_sqrt_pd(_mm256_fmadd_pd(vx, vx, _mm256_mul_pd(vy, vy)));
    __m256d cond = _mm256_cmp_pd(_mm256_sqrt_pd(_mm256_hadd_pd(_mm256_mul_pd(vx, vx), cst_0)), 
                                 _mm256_sqrt_pd(_mm256_hadd_pd(_mm256_mul_pd(vy, vy), cst_0)), _CMP_GT_OQ);

    unsigned int k = (unsigned int)(z[i] * 4.0);
    double t0, t1;
    switch (k) {
      case 0:
        t0 = x[i * 2] + y[i * 2] * 5.1;
        t1 = x[i * 2 + 1] + y[i * 2 + 1] * 5.1;
        break;
      case 1:
        t0 = (x[i * 2]) * (y[i * 2] * 2.1) - (x[i * 2 + 1]) * (y[i * 2 + 1] * 2.1);
        t1 = (x[i * 2]) * (y[i * 2 + 1] * 2.1) + (x[i * 2 + 1]) * (y[i * 2] * 2.1);
        break;
      case 2:
        t0 = x[i * 2] + y[i * 2] * (-3.5);
        t1 = x[i * 2 + 1] + y[i * 2 + 1] * (-3.5);
        break;
      case 3:
        t0 = (x[i * 2]) * (y[i * 2] * (-4.1)) - (x[i * 2 + 1]) * (y[i * 2 + 1] * (-4.1));
        t1 = (x[i * 2]) * (y[i * 2 + 1] * (-4.1)) + (x[i * 2 + 1]) * (y[i * 2] * (-4.1));
        break;
      default:
        t0 = t1 = 0.0;
        break;
    }

    __m256d vt = _mm256_set_pd(0.0, 0.0, t1, t0);
    __m256d vres = _mm256_blendv_pd(_mm256_add_pd(_mm256_hadd_pd(_mm256_mul_pd(vx, vx), cst_0), _mm256_hadd_pd(_mm256_mul_pd(vy, vy), cst_0)),
                                    _mm256_add_pd(_mm256_set1_pd(z[i]), _mm256_hadd_pd(vt, _mm256_sub_pd(_mm256_mul_pd(_mm256_permute_pd(vt, 5), _mm256_permute_pd(vt, 5)), _mm256_mul_pd(vt, vt)))), cond);
    _mm_store_sd(&z[i], _mm256_castpd256_pd128(vres));
  }
}

>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance1
9477.59 cycles

Running: maxperformance
9429.26 cycles
<<<
WA
================================