Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include <math.h>
#include "common.h"

// Precondition : 0 <= y[i] < 1.0, n = 1024
void slow_performance(double *__restrict x, double *__restrict y, int n)
{
    for (int i = 0; i < n; i++)
    {
        unsigned int k = floor(4.0 * y[i]);

        double real = x[2 * i];
        double imag = x[2 * i + 1];

        double real_sqr = real * real - imag * imag;
        double imag_sqr = 2.0 * real * imag;

        switch (k)
        {
        case 0:
            y[i] += fmin(real_sqr, imag_sqr);
            break;
        case 1:
            y[i] += fmax(real_sqr, imag_sqr);
            break;
        default:
            y[i] += real * real + imag * imag;
            break;
        }
    }
}

void maxperformance(double *__restrict x, double *__restrict y, int n)
{
}
```


================================
```cpp
#include <immintrin.h>
#include <x86intrin.h>
#include <stdint.h>
#include <stddef.h>
#include <math.h>
#include "common.h"

inline __m256d min256(__m256d a, __m256d b) {
    return _mm256_min_pd(a, b);
}

inline __m256d max256(__m256d a, __m256d b) {
    return _mm256_max_pd(a, b);
}

void maxperformance(double *__restrict x, double *__restrict y, int n)
{
    const __m256d four = _mm256_set1_pd(4.0);
    const __m256d two = _mm256_set1_pd(2.0);
    const __m256i mask_2 = _mm256_set1_epi64x(0x1);
    
    int i = 0;
    for (; i <= n - 4; i += 4)
    {
        // Load y
        __m256d y_vec = _mm256_load_pd(y + i);
        __m256d y_scaled = _mm256_mul_pd(four, y_vec);
        __m256i k_vec = _mm256_cvttpd_epi32(y_scaled);

        // Load x (real and imag interleaved)
        __m256d real = _mm256_set_pd(x[2*i+6], x[2*i+4], x[2*i+2], x[2*i+0]);
        __m256d imag = _mm256_set_pd(x[2*i+7], x[2*i+5], x[2*i+3], x[2*i+1]);

        // Compute squares
        __m256d real_sqr = _mm256_mul_pd(real, real);
        __m256d imag_sqr = _mm256_mul_pd(imag, imag);
        __m256d real_sq = _mm256_sub_pd(real_sqr, imag_sqr);
        __m256d imag_sq = _mm256_mul_pd(_mm256_mul_pd(real, imag), two);
        __m256d mag_sq = _mm256_add_pd(real_sqr, imag_sqr);

        // Create masks
        __m256i cmp0 = _mm256_cmpeq_epi32(k_vec, _mm256_set1_epi32(0));
        __m256i cmp1 = _mm256_cmpeq_epi32(k_vec, _mm256_set1_epi32(1));
        __m256d m0 = _mm256_castsi256_pd(cmp0);
        __m256d m1 = _mm256_castsi256_pd(cmp1);
        __m256d m2 = _mm256_andnot_pd(_mm256_or_pd(m0, m1), _mm256_castsi256_pd(_mm256_set1_epi64x(-1)));

        // Select results
        __m256d min_val = min256(real_sq, imag_sq);
        __m256d max_val = max256(real_sq, imag_sq);
        __m256d res0 = _mm256_blendv_pd(_mm256_setzero_pd(), min_val, m0);
        __m256d res1 = _mm256_blendv_pd(_mm256_setzero_pd(), max_val, m1);
        __m256d res2 = _mm256_blendv_pd(_mm256_setzero_pd(), mag_sq, m2);
        __m256d result = _mm256_add_pd(_mm256_add_pd(res0, res1), res2);

        // Store result
        __m256d y_out = _mm256_add_pd(y_vec, result);
        _mm256_store_pd(y + i, y_out);
    }

    // Handle tail elements
    for (; i < n; i++)
    {
        unsigned int k = (unsigned int)(4.0 * y[i]);

        double real = x[2 * i];
        double imag = x[2 * i + 1];

        double real_sqr = real * real - imag * imag;
        double imag_sqr = 2.0 * real * imag;

        switch (k)
        {
        case 0:
            y[i] += real_sqr < imag_sqr ? real_sqr : imag_sqr;
            break;
        case 1:
            y[i] += real_sqr > imag_sqr ? real_sqr : imag_sqr;
            break;
        default:
            y[i] += real * real + imag * imag;
            break;
        }
    }
}
```
>>>
make: *** [Makefile:13: benchmark] Error 1
<<<
CE
================================
```cpp
#include <immintrin.h>
#include <stdint.h>
#include <x86intrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict y, int n)
{
    const __m256d four = _mm256_set1_pd(4.0);
    const __m256d two = _mm256_set1_pd(2.0);

    for (int i = 0; i < n; i += 4)
    {
        __m256d yv = _mm256_load_pd(y + i);
        __m256d y4 = _mm256_mul_pd(yv, four);
        __m128i ki = _mm_cvttps_epi32(_mm256_castpd256_pd128(y4));

        __m256d real0 = _mm256_set_pd(x[2 * (i + 3)], x[2 * (i + 2)], x[2 * (i + 1)], x[2 * (i + 0)]);
        __m256d imag0 = _mm256_set_pd(x[2 * (i + 3) + 1], x[2 * (i + 2) + 1], x[2 * (i + 1) + 1], x[2 * (i + 0) + 1]);

        __m256d real_sqr = _mm256_sub_pd(_mm256_mul_pd(real0, real0), _mm256_mul_pd(imag0, imag0));
        __m256d imag_sqr = _mm256_mul_pd(two, _mm256_mul_pd(real0, imag0));
        __m256d abs_sqr  = _mm256_add_pd(_mm256_mul_pd(real0, real0), _mm256_mul_pd(imag0, imag0));

        alignas(32) uint32_t kbuf[4];
        _mm_storeu_si128((__m128i*)kbuf, ki);

        alignas(32) double real_sqr_buf[4], imag_sqr_buf[4], abs_sqr_buf[4], y_buf[4];
        _mm256_store_pd(real_sqr_buf, real_sqr);
        _mm256_store_pd(imag_sqr_buf, imag_sqr);
        _mm256_store_pd(abs_sqr_buf, abs_sqr);
        _mm256_store_pd(y_buf, yv);

        for (int j = 0; j < 4; ++j)
        {
            switch (kbuf[j])
            {
            case 0:
                y_buf[j] += (real_sqr_buf[j] < imag_sqr_buf[j]) ? real_sqr_buf[j] : imag_sqr_buf[j];
                break;
            case 1:
                y_buf[j] += (real_sqr_buf[j] > imag_sqr_buf[j]) ? real_sqr_buf[j] : imag_sqr_buf[j];
                break;
            default:
                y_buf[j] += abs_sqr_buf[j];
                break;
            }
        }

        _mm256_store_pd(y + i, _mm256_load_pd(y_buf));
    }
}
```
>>>
make: *** [Makefile:13: benchmark] Error 1
<<<
CE
================================
```cpp
#include <immintrin.h>
#include <algorithm>
#include <stdint.h>

inline __m256d min256(__m256d a, __m256d b) {
    return _mm256_min_pd(a, b);
}

inline __m256d max256(__m256d a, __m256d b) {
    return _mm256_max_pd(a, b);
}

void maxperformance(double *__restrict x, double *__restrict y, int n)
{
    const __m256d two = _mm256_set1_pd(2.0);
    const __m256d four = _mm256_set1_pd(4.0);

    for (int i = 0; i < n; i += 4)
    {
        // Load y[i] and compute k = floor(4.0 * y[i])
        __m256d y_vec = _mm256_load_pd(y + i);
        __m256d ky = _mm256_mul_pd(y_vec, four);
        __m256i k = _mm256_cvttpd_epi32(ky);

        // Load x[2*i] and x[2*i+1] into real and imag
        __m256d real = _mm256_set_pd(x[2*(i+3)], x[2*(i+2)], x[2*(i+1)], x[2*(i)]);
        __m256d imag = _mm256_set_pd(x[2*(i+3)+1], x[2*(i+2)+1], x[2*(i+1)+1], x[2*(i)+1]);

        // Compute real_sqr = real*real - imag*imag
        __m256d real_sq = _mm256_mul_pd(real, real);
        __m256d imag_sq = _mm256_mul_pd(imag, imag);
        __m256d real_sqr = _mm256_sub_pd(real_sq, imag_sq);

        // Compute imag_sqr = 2.0 * real * imag
        __m256d real_imag = _mm256_mul_pd(real, imag);
        __m256d imag_sqr = _mm256_mul_pd(two, real_imag);

        // Compute magnitude = real^2 + imag^2
        __m256d magnitude = _mm256_add_pd(real_sq, imag_sq);

        // Blend results based on k
        __m256d min_val = min256(real_sqr, imag_sqr);
        __m256d max_val = max256(real_sqr, imag_sqr);

        // Convert k to double for comparison
        __m256d k_pd = _mm256_cvtepi32_pd(k);

        // case 0: k == 0
        __m256d mask0 = _mm256_cmp_pd(k_pd, _mm256_set1_pd(0.0), _CMP_EQ_OQ);
        // case 1: k == 1
        __m256d mask1 = _mm256_cmp_pd(k_pd, _mm256_set1_pd(1.0), _CMP_EQ_OQ);
        // default: (k != 0 && k != 1)
        __m256d maskd = _mm256_or_pd(mask0, mask1);
        maskd = _mm256_xor_pd(maskd, _mm256_castsi256_pd(_mm256_set1_epi64x(-1)));

        // Select result based on k
        __m256d update0 = _mm256_and_pd(mask0, min_val);
        __m256d update1 = _mm256_and_pd(mask1, max_val);
        __m256d updated = _mm256_or_pd(update0, update1);
        updated = _mm256_or_pd(updated, _mm256_and_pd(maskd, magnitude));

        // y[i] += updated
        __m256d result = _mm256_add_pd(y_vec, updated);
        _mm256_store_pd(y + i, result);
    }
}
```
>>>
make: *** [Makefile:13: benchmark] Error 1
<<<
CE
================================