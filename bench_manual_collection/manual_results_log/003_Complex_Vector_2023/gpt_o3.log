Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` function—no other text, comments, or explanations.


```
#include <immintrin.h>
#include <math.h>
#include "common.h"

// Precondition : 0 <= y[i] < 1.0, n = 1024
void slow_performance(double *__restrict x, double *__restrict y, int n)
{
    for (int i = 0; i < n; i++)
    {
        unsigned int k = floor(4.0 * y[i]);

        double real = x[2 * i];
        double imag = x[2 * i + 1];

        double real_sqr = real * real - imag * imag;
        double imag_sqr = 2.0 * real * imag;

        switch (k)
        {
        case 0:
            y[i] += fmin(real_sqr, imag_sqr);
            break;
        case 1:
            y[i] += fmax(real_sqr, imag_sqr);
            break;
        default:
            y[i] += real * real + imag * imag;
            break;
        }
    }
}

void maxperformance(double *__restrict x, double *__restrict y, int n)
{
}
```


================================
```cpp
void max_performance(double *__restrict x, double *__restrict y, int n)
{
    const __m256d v_one  = _mm256_set1_pd(1.0);
    const __m256d v_two  = _mm256_set1_pd(2.0);
    const __m256d v_four = _mm256_set1_pd(4.0);

    int i = 0;
    for (; i + 3 < n; i += 4)
    {
        /* --- load y and build masks for k ------------------------------ */
        __m256d yv     = _mm256_loadu_pd(y + i);
        __m256d four_y = _mm256_mul_pd(yv, v_four);

        __m256d mask_k0 = _mm256_cmp_pd(four_y, v_one, _CMP_LT_OQ);                                   // k == 0
        __m256d mask_k1 = _mm256_and_pd(_mm256_cmp_pd(four_y, v_one, _CMP_GE_OQ),                     // k == 1
                                         _mm256_cmp_pd(four_y, v_two, _CMP_LT_OQ));

        /* --- load 4 complex values (8 doubles) from x ------------------ */
        const double *xp = x + (i << 1);                   // 2*i
        __m256d v0 = _mm256_loadu_pd(xp);                  // r0 i0 r1 i1
        __m256d v1 = _mm256_loadu_pd(xp + 4);              // r2 i2 r3 i3

        /* de-interleave to get real = [r0 r1 r2 r3], imag = [i0 i1 i2 i3] */
        __m256d lo   = _mm256_unpacklo_pd(v0, v1);         // r0 r2 r1 r3
        __m256d hi   = _mm256_unpackhi_pd(v0, v1);         // i0 i2 i1 i3
        __m256d real = _mm256_permute4x64_pd(lo, 0b11011000);
        __m256d imag = _mm256_permute4x64_pd(hi, 0b11011000);

        /* --- compute required quantities ------------------------------ */
        __m256d real_sq = _mm256_mul_pd(real, real);
        __m256d imag_sq = _mm256_mul_pd(imag, imag);

        __m256d real_sqr = _mm256_sub_pd(real_sq, imag_sq);                 // real^2 − imag^2
        __m256d imag_sqr = _mm256_mul_pd(_mm256_mul_pd(real, imag), v_two); // 2·real·imag
        __m256d mag_sq   = _mm256_add_pd(real_sq, imag_sq);                 // |z|^2

        /* --- select update term according to k ------------------------ */
        __m256d upd = mag_sq;                                   /* default (k ≥ 2) */
        upd = _mm256_blendv_pd(upd, real_sqr, mask_k1);         /* k == 1 → max   */
        upd = _mm256_blendv_pd(upd, imag_sqr, mask_k0);         /* k == 0 → min   */

        /* --- y[i] += upd --------------------------------------------- */
        yv = _mm256_add_pd(yv, upd);
        _mm256_storeu_pd(y + i, yv);
    }

    /* ---------------------------- scalar tail ------------------------- */
    for (; i < n; ++i)
    {
        unsigned int k = (unsigned int)(4.0 * y[i]);   /* 0 ≤ y[i] < 1 ⇒ k ∈ {0,1,2,3} */

        double real = x[2 * i];
        double imag = x[2 * i + 1];

        double real_sqr = real * real - imag * imag;
        double imag_sqr = 2.0 * real * imag;

        if (k == 0)
            y[i] += (real_sqr < imag_sqr) ? real_sqr : imag_sqr;
        else if (k == 1)
            y[i] += (real_sqr > imag_sqr) ? real_sqr : imag_sqr;
        else
            y[i] += real * real + imag * imag;
    }
}
```
>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance
7009.66 cycles

Running: maxperformance
1061.52 cycles
<<<
WA
================================
```cpp
void max_performance(double *__restrict x, double *__restrict y, int n)
{
    const __m256d v_quarter = _mm256_set1_pd(0.25);
    const __m256d v_half    = _mm256_set1_pd(0.5);
    const __m256d v_two     = _mm256_set1_pd(2.0);
    __m256i idx_real = _mm256_set_epi64x(6, 4, 2, 0);
    __m256i idx_imag = _mm256_set_epi64x(7, 5, 3, 1);
    const __m256i step = _mm256_set1_epi64x(8);

    int i = 0;
    for (; i <= n - 4; i += 4)
    {
        __m256d yv     = _mm256_loadu_pd(y + i);
        __m256d realv  = _mm256_i64gather_pd(x, idx_real, 8);
        __m256d imagv  = _mm256_i64gather_pd(x, idx_imag, 8);
        idx_real = _mm256_add_epi64(idx_real, step);
        idx_imag = _mm256_add_epi64(idx_imag, step);

        __m256d real2  = _mm256_mul_pd(realv, realv);
        __m256d imag2  = _mm256_mul_pd(imagv, imagv);
        __m256d sumv   = _mm256_add_pd(real2, imag2);
        __m256d diffv  = _mm256_sub_pd(real2, imag2);
        __m256d crossv = _mm256_mul_pd(_mm256_mul_pd(realv, imagv), v_two);

        __m256d minv   = _mm256_min_pd(diffv, crossv);
        __m256d maxv   = _mm256_max_pd(diffv, crossv);

        __m256d mask0  = _mm256_cmp_pd(yv, v_quarter, _CMP_LT_OQ);
        __m256d maskLt = _mm256_cmp_pd(yv, v_half,    _CMP_LT_OQ);
        __m256d mask1  = _mm256_andnot_pd(mask0, maskLt);

        __m256d res    = sumv;
        res = _mm256_blendv_pd(res, maxv, mask1);
        res = _mm256_blendv_pd(res, minv, mask0);

        _mm256_storeu_pd(y + i, _mm256_add_pd(yv, res));
    }

    for (; i < n; ++i)
    {
        unsigned int k = (unsigned int)(y[i] * 4.0);
        double real = x[2 * i];
        double imag = x[2 * i + 1];
        double real2 = real * real;
        double imag2 = imag * imag;
        double diff  = real2 - imag2;
        double cross = 2.0 * real * imag;

        switch (k)
        {
        case 0: y[i] += (diff < cross ? diff : cross); break;
        case 1: y[i] += (diff > cross ? diff : cross); break;
        default: y[i] += real2 + imag2; break;
        }
    }
}
```
>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance
6591.2 cycles

Running: maxperformance
4150.41 cycles
<<<
WA
================================
```cpp
void max_performance(double *__restrict x, double *__restrict y, int n)
{
    const __m256i idx_r = _mm256_set_epi64x(48, 32, 16, 0);  // offsets for real parts
    const __m256i idx_i = _mm256_set_epi64x(56, 40, 24, 8);  // offsets for imag parts

    const __m256d one  = _mm256_set1_pd(1.0);
    const __m256d two  = _mm256_set1_pd(2.0);
    const __m256d four = _mm256_set1_pd(4.0);

    for (int i = 0; i < n; i += 4)
    {
        double *px = x + (i << 1);                 // points to 2*i

        // Gather real and imaginary components
        __m256d re = _mm256_i64gather_pd(px, idx_r, 1);
        __m256d im = _mm256_i64gather_pd(px, idx_i, 1);

        // Load y
        __m256d yv = _mm256_loadu_pd(y + i);

        // Compute squares and products
        __m256d re2   = _mm256_mul_pd(re, re);
        __m256d im2   = _mm256_mul_pd(im, im);
        __m256d sum   = _mm256_add_pd(re2, im2);                     // re^2 + im^2
        __m256d diff  = _mm256_sub_pd(re2, im2);                     // re^2 - im^2
        __m256d prod  = _mm256_mul_pd(re, im);                       // re*im
        __m256d prod2 = _mm256_add_pd(prod, prod);                   // 2*re*im

        // min / max of the two branch candidates
        __m256d vmin = _mm256_min_pd(diff, prod2);
        __m256d vmax = _mm256_max_pd(diff, prod2);

        // Prepare candidate results
        __m256d add0 = _mm256_add_pd(yv, vmin); // k == 0
        __m256d add1 = _mm256_add_pd(yv, vmax); // k == 1
        __m256d add2 = _mm256_add_pd(yv, sum);  // k >= 2

        // Determine k via y*4 ranges   (k = floor(4*y))
        __m256d y4   = _mm256_mul_pd(yv, four);                 // 4*y
        __m256d m0   = _mm256_cmp_pd(y4, one, _CMP_LT_OQ);      // y4 < 1  -> k==0
        __m256d lt2  = _mm256_cmp_pd(y4, two, _CMP_LT_OQ);      // y4 < 2
        __m256d m1   = _mm256_andnot_pd(m0, lt2);               // 1 <= y4 < 2 -> k==1

        // Blend results: start with default (k>=2), then overlay k==1, then k==0
        __m256d res  = add2;
        res          = _mm256_blendv_pd(res, add1, m1);
        res          = _mm256_blendv_pd(res, add0, m0);

        _mm256_storeu_pd(y + i, res);
    }
}
```
>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance
6893.34 cycles

Running: maxperformance
2246.15 cycles
<<<
WA
================================