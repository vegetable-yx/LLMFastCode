Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
void slow_performance1(double *__restrict x, double *__restrict y)
{
    for (int i = 0; i < 4; i++)
    {
        for (int j = 0; j < 4; j++)
        {
            y[i * 8 + j * 2] = x[j * 8 + i * 2];
            y[i * 8 + j * 2 + 1] = -x[j * 8 + i * 2 + 1];
        }
    }
}

void max_performance(double *__restrict x, double *__restrict y)
{
}
```


================================
#include <immintrin.h>

void max_performance(double *__restrict x, double *__restrict y)
{
    for (int i = 0; i < 4; ++i) {
        double* y_base = y + i * 8;
        // Load x values for j=0 to 3
        double x0 = x[0*8 + i*2];
        double x1 = x[1*8 + i*2];
        double x2 = x[2*8 + i*2];
        double x3 = x[3*8 + i*2];
        double x0_1 = x[0*8 + i*2 + 1];
        double x1_1 = x[1*8 + i*2 + 1];
        double x2_1 = x[2*8 + i*2 + 1];
        double x3_1 = x[3*8 + i*2 + 1];
        
        // Create vectors with even elements and negated odd elements
        __m256d v1 = _mm256_setr_pd(x0, -x0_1, x1, -x1_1);
        __m256d v2 = _mm256_setr_pd(x2, -x2_1, x3, -x3_1);
        
        // Store vectors to consecutive y positions
        _mm256_storeu_pd(y_base, v1);
        _mm256_storeu_pd(y_base + 4, v2);
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance1
26.5104 cycles

Running: max_performance
8.928 cycles
<<<
26.5104 / 8.928 = 2.96935484
================================
void maxperformance(double *__restrict x, double *__restrict y)
{
    for (int j = 0; j < 4; ++j) {
        for (int i = 0; i < 4; ++i) {
            const int x_idx = 8 * j + 2 * i;
            const int y_idx = 8 * i + 2 * j;
            y[y_idx] = x[x_idx];
            y[y_idx + 1] = -x[x_idx + 1];
        }
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance1
27.0269 cycles

Running: max_performance
20.4709 cycles
<<<
27.0269 / 20.4709 = 1.32025949
================================
#include <immintrin.h>

void max_performance(double *__restrict x, double *__restrict y)
{
    for (int i = 0; i < 4; ++i) {
        const int idx = 2 * i;
        const double* x_ptr = x + idx;
        
        __m256d vec0 = _mm256_setr_pd(
            x_ptr[0*8],   -x_ptr[0*8 + 1],
            x_ptr[1*8],   -x_ptr[1*8 + 1]
        );
        __m256d vec1 = _mm256_setr_pd(
            x_ptr[2*8],   -x_ptr[2*8 + 1],
            x_ptr[3*8],   -x_ptr[3*8 + 1]
        );
        
        double* y_ptr = y + 8 * i;
        _mm256_store_pd(y_ptr, vec0);
        _mm256_store_pd(y_ptr + 4, vec1);
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance1
26.9558 cycles

Running: max_performance
8.96789 cycles
<<<
26.9558 / 8.96789 = 3.00581296
================================