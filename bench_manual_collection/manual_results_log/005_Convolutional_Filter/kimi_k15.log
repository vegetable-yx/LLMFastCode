Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include "common.h"

#define K 4
#define N 128

#define GET_X(i, j) x[(i) * N + (j)]
#define GET_Y(i, j) y[(i) * ON + (j)]
#define GET_H(i, j) h[(i) * K + (j)]

void slow_performance(double *__restrict x, double *__restrict h, double *__restrict y)
{
    const int ON = N - K + 1;
    for (int i = 0; i < ON; i++)
    {
        for (int j = 0; j < ON; j++)
        {
            GET_Y(i, j) = 0.0;
            for (int k0 = 0; k0 < K; k0++)
            {
                for (int k1 = 0; k1 < K; k1++)
                {
                    GET_Y(i, j) += (i + j + k0 + k1 - 1) * GET_H(k0, k1) * fabs(GET_X(i + k0, j + k1));
                }
            }
        }
    }
}

void max_performance(double *__restrict x, double *__restrict h, double *__restrict y)
{
}
```


================================
#include <immintrin.h>
#include "common.h"

#define K 4
#define N 128

#define GET_X(i, j) x[(i) * N + (j)]
#define GET_Y(i, j) y[(i) * ON + (j)]
#define GET_H(i, j) h[(i) * K + (j)]

void max_performance(double *__restrict x, double *__restrict h, double *__restrict y)
{
    const int ON = N - K + 1;
    const __m256d sign_mask = _mm256_set1_pd(-0.0);

    for (int i = 0; i < ON; i++)
    {
        for (int j = 0; j < ON; j += 4)
        {
            __m256d y_vec = _mm256_setzero_pd();
            for (int k0 = 0; k0 < K; k0++)
            {
                __m256d x_vec[K];
                for (int k1 = 0; k1 < K; k1++)
                {
                    x_vec[k1] = _mm256_andnot_pd(sign_mask, _mm256_loadu_pd(&GET_X(i + k0, j + k1)));
                }
                __m256d h_vec0 = _mm256_broadcast_sd(&GET_H(k0, 0));
                __m256d h_vec1 = _mm256_broadcast_sd(&GET_H(k0, 1));
                __m256d h_vec2 = _mm256_broadcast_sd(&GET_H(k0, 2));
                __m256d h_vec3 = _mm256_broadcast_sd(&GET_H(k0, 3));

                y_vec = _mm256_fmadd_pd(_mm256_set1_pd(i + j + k0 - 1), _mm256_fmadd_pd(h_vec0, x_vec[0], _mm256_fmadd_pd(h_vec1, x_vec[1], _mm256_fmadd_pd(h_vec2, x_vec[2], _mm256_mul_pd(h_vec3, x_vec[3])))), y_vec);
            }
            _mm256_storeu_pd(&GET_Y(i, j), y_vec);
        }
    }
}

>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance1
71401.9 cycles

Running: maxperformance
51371.1 cycles
<<<
WA
================================
#include <immintrin.h>
#include "common.h"

#define K 4
#define N 128

#define GET_X(i, j) x[(i) * N + (j)]
#define GET_Y(i, j) y[(i) * ON + (j)]
#define GET_H(i, j) h[(i) * K + (j)]

void max_performance(double *__restrict x, double *__restrict h, double *__restrict y)
{
    const int ON = N - K + 1;
    __m256i sign_mask = _mm256_set1_epi64x(0x8000000000000000);
    __m256d increment = _mm256_setr_pd(0.0, 1.0, 2.0, 3.0);
    __m256d h_vec0 = _mm256_loadu_pd(h + 0*K);
    __m256d h_vec1 = _mm256_loadu_pd(h + 1*K);
    __m256d h_vec2 = _mm256_loadu_pd(h + 2*K);
    __m256d h_vec3 = _mm256_loadu_pd(h + 3*K);

    for (int i = 0; i < ON; i++) {
        for (int j = 0; j < ON; j++) {
            double* y_ptr = y + i*ON + j;
            *y_ptr = 0.0;
            double base = i + j - 1;

            // k0=0
            {
                double* x_ptr = x + (i + 0)*N + j;
                __m256d x_vec = _mm256_loadu_pd(x_ptr);
                __m256d abs_x = _mm256_andnot_pd(_mm256_castsi256_pd(sign_mask), x_vec);
                __m256d coeff = _mm256_add_pd(_mm256_set1_pd(base + 0), increment);
                __m256d product = _mm256_mul_pd(_mm256_mul_pd(coeff, h_vec0), abs_x);
                double sum = ((double*)&product)[0] + ((double*)&product)[1] + ((double*)&product)[2] + ((double*)&product)[3];
                *y_ptr += sum;
            }

            // k0=1
            {
                double* x_ptr = x + (i + 1)*N + j;
                __m256d x_vec = _mm256_loadu_pd(x_ptr);
                __m256d abs_x = _mm256_andnot_pd(_mm256_castsi256_pd(sign_mask), x_vec);
                __m256d coeff = _mm256_add_pd(_mm256_set1_pd(base + 1), increment);
                __m256d product = _mm256_mul_pd(_mm256_mul_pd(coeff, h_vec1), abs_x);
                double sum = ((double*)&product)[0] + ((double*)&product)[1] + ((double*)&product)[2] + ((double*)&product)[3];
                *y_ptr += sum;
            }

            // k0=2
            {
                double* x_ptr = x + (i + 2)*N + j;
                __m256d x_vec = _mm256_loadu_pd(x_ptr);
                __m256d abs_x = _mm256_andnot_pd(_mm256_castsi256_pd(sign_mask), x_vec);
                __m256d coeff = _mm256_add_pd(_mm256_set1_pd(base + 2), increment);
                __m256d product = _mm256_mul_pd(_mm256_mul_pd(coeff, h_vec2), abs_x);
                double sum = ((double*)&product)[0] + ((double*)&product)[1] + ((double*)&product)[2] + ((double*)&product)[3];
                *y_ptr += sum;
            }

            // k0=3
            {
                double* x_ptr = x + (i + 3)*N + j;
                __m256d x_vec = _mm256_loadu_pd(x_ptr);
                __m256d abs_x = _mm256_andnot_pd(_mm256_castsi256_pd(sign_mask), x_vec);
                __m256d coeff = _mm256_add_pd(_mm256_set1_pd(base + 3), increment);
                __m256d product = _mm256_mul_pd(_mm256_mul_pd(coeff, h_vec3), abs_x);
                double sum = ((double*)&product)[0] + ((double*)&product)[1] + ((double*)&product)[2] + ((double*)&product)[3];
                *y_ptr += sum;
            }
        }
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance1
71045.7 cycles

Running: maxperformance
237340 cycles
<<<
0.2993414510828347
================================
#include <immintrin.h>
#include "common.h"

#define K 4
#define N 128

static const __m256d k0_plus_k1_vec[4] = {
    _mm256_setr_pd(0.0, 1.0, 2.0, 3.0),
    _mm256_setr_pd(1.0, 2.0, 3.0, 4.0),
    _mm256_setr_pd(2.0, 3.0, 4.0, 5.0),
    _mm256_setr_pd(3.0, 4.0, 5.0, 6.0)
};

double hsum(__m256d v) {
    v = _mm256_hadd_pd(v, v);
    v = _mm256_hadd_pd(v, v);
    return _mm256_extract_pd(v, 0);
}

void max_performance(double *__restrict x, double *__restrict h, double *__restrict y)
{
    const int ON = N - K + 1;
    __m256d h_vecs[K];
    for (int k0 = 0; k0 < K; ++k0)
        h_vecs[k0] = _mm256_loadu_pd(h + k0*K);
    
    for (int i = 0; i < ON; ++i) {
        for (int j = 0; j < ON; ++j) {
            double sum1 = 0.0, sum2 = 0.0;
            for (int k0 = 0; k0 < K; ++k0) {
                __m256d x_vec = _mm256_loadu_pd(x + (i + k0)*N + j);
                __m256d x_abs = _mm256_abs_pd(x_vec);
                __m256d temp1 = _mm256_mul_pd(x_abs, h_vecs[k0]);
                sum1 += hsum(temp1);
                __m256d coeff_vec = k0_plus_k1_vec[k0];
                __m256d temp2 = _mm256_mul_pd(temp1, coeff_vec);
                sum2 += hsum(temp2);
            }
            y[i*ON + j] = (i + j - 1) * sum1 + sum2;
        }
    }
}
>>>
make: *** [Makefile:13: benchmark] Error 1
<<<
CE
================================