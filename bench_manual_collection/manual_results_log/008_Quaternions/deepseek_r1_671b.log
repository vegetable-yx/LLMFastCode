Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include "common.h"

void slow_performance(double*__restrict x, double*__restrict y, double*__restrict A) {
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 2; j++) {
            double a = x[4 * i + 0];
            double b = x[4 * i + 1];
            double c = x[4 * i + 2];
            double d = x[4 * i + 3];

            double e = y[4 * j + 0];
            double f = y[4 * j + 1];
            double g = y[4 * j + 2];
            double h = y[4 * j + 3];

            // Quaternion multiplication:
            // (a + bi + cj + dk) * (e + fi + gj + hk) =
            // (ae - bf - cg - dh) +
            // (af + be + ch - dg)i +
            // (ag - bh + ce + df)j +
            // (ah + bg - cf + de)k
            double r0 = a * e - b * f - c * g - d * h;
            double r1 = a * f + b * e + c * h - d * g;
            double r2 = a * g - b * h + c * e + d * f;
            double r3 = a * h + b * g - c * f + d * e;

            int index = 4 * (i * 2 + j);
            A[index + 0] = r0;
            A[index + 1] = r1;
            A[index + 2] = r2;
            A[index + 3] = r3;
        }
    }
}

void max_performance(double*__restrict x, double*__restrict y, double*__restrict A)
{
}
```


================================
#include <immintrin.h>
#include "common.h"

inline double hsum_avx(__m256d v) {
    __m128d vlow = _mm256_castpd256_pd128(v);
    __m128d vhigh = _mm256_extractf128_pd(v, 1);
    vlow = _mm_add_pd(vlow, vhigh);
    __m128d high64 = _mm_unpackhi_pd(vlow, vlow);
    return _mm_cvtsd_f64(_mm_add_sd(vlow, high64));
}

void max_performance(double* __restrict x, double* __restrict y, double* __restrict A) {
    __m256d x0 = _mm256_load_pd(x);
    __m256d x1 = _mm256_load_pd(x + 4);
    __m256d y0 = _mm256_load_pd(y);
    __m256d y1 = _mm256_load_pd(y + 4);

    // x0 * y0 -> A[0-3]
    __m256d p0 = _mm256_mul_pd(x0, _mm256_mul_pd(y0, _mm256_setr_pd(1, -1, -1, -1)));
    __m256d p1 = _mm256_mul_pd(x0, _mm256_mul_pd(_mm256_permute4x64_pd(y0, _MM_SHUFFLE(2,3,0,1)), _mm256_setr_pd(1, 1, 1, -1)));
    __m256d p2 = _mm256_mul_pd(x0, _mm256_mul_pd(_mm256_permute4x64_pd(y0, _MM_SHUFFLE(1,0,3,2)), _mm256_setr_pd(1, -1, 1, 1)));
    __m256d p3 = _mm256_mul_pd(x0, _mm256_mul_pd(_mm256_permute4x64_pd(y0, _MM_SHUFFLE(0,1,2,3)), _mm256_setr_pd(1, 1, -1, 1)));
    _mm_store_pd(A, _mm_setr_pd(hsum_avx(p0), hsum_avx(p1)));
    _mm_store_pd(A+2, _mm_setr_pd(hsum_avx(p2), hsum_avx(p3)));

    // x0 * y1 -> A[4-7]
    __m256d p4 = _mm256_mul_pd(x0, _mm256_mul_pd(y1, _mm256_setr_pd(1, -1, -1, -1)));
    __m256d p5 = _mm256_mul_pd(x0, _mm256_mul_pd(_mm256_permute4x64_pd(y1, _MM_SHUFFLE(2,3,0,1)), _mm256_setr_pd(1, 1, 1, -1)));
    __m256d p6 = _mm256_mul_pd(x0, _mm256_mul_pd(_mm256_permute4x64_pd(y1, _MM_SHUFFLE(1,0,3,2)), _mm256_setr_pd(1, -1, 1, 1)));
    __m256d p7 = _mm256_mul_pd(x0, _mm256_mul_pd(_mm256_permute4x64_pd(y1, _MM_SHUFFLE(0,1,2,3)), _mm256_setr_pd(1, 1, -1, 1)));
    _mm_store_pd(A+4, _mm_setr_pd(hsum_avx(p4), hsum_avx(p5)));
    _mm_store_pd(A+6, _mm_setr_pd(hsum_avx(p6), hsum_avx(p7)));

    // x1 * y0 -> A[8-11]
    __m256d p8 = _mm256_mul_pd(x1, _mm256_mul_pd(y0, _mm256_setr_pd(1, -1, -1, -1)));
    __m256d p9 = _mm256_mul_pd(x1, _mm256_mul_pd(_mm256_permute4x64_pd(y0, _MM_SHUFFLE(2,3,0,1)), _mm256_setr_pd(1, 1, 1, -1)));
    __m256d p10 = _mm256_mul_pd(x1, _mm256_mul_pd(_mm256_permute4x64_pd(y0, _MM_SHUFFLE(1,0,3,2)), _mm256_setr_pd(1, -1, 1, 1)));
    __m256d p11 = _mm256_mul_pd(x1, _mm256_mul_pd(_mm256_permute4x64_pd(y0, _MM_SHUFFLE(0,1,2,3)), _mm256_setr_pd(1, 1, -1, 1)));
    _mm_store_pd(A+8, _mm_setr_pd(hsum_avx(p8), hsum_avx(p9)));
    _mm_store_pd(A+10, _mm_setr_pd(hsum_avx(p10), hsum_avx(p11)));

    // x1 * y1 -> A[12-15]
    __m256d p12 = _mm256_mul_pd(x1, _mm256_mul_pd(y1, _mm256_setr_pd(1, -1, -1, -1)));
    __m256d p13 = _mm256_mul_pd(x1, _mm256_mul_pd(_mm256_permute4x64_pd(y1, _MM_SHUFFLE(2,3,0,1)), _mm256_setr_pd(1, 1, 1, -1)));
    __m256d p14 = _mm256_mul_pd(x1, _mm256_mul_pd(_mm256_permute4x64_pd(y1, _MM_SHUFFLE(1,0,3,2)), _mm256_setr_pd(1, -1, 1, 1)));
    __m256d p15 = _mm256_mul_pd(x1, _mm256_mul_pd(_mm256_permute4x64_pd(y1, _MM_SHUFFLE(0,1,2,3)), _mm256_setr_pd(1, 1, -1, 1)));
    _mm_store_pd(A+12, _mm_setr_pd(hsum_avx(p12), hsum_avx(p13)));
    _mm_store_pd(A+14, _mm_setr_pd(hsum_avx(p14), hsum_avx(p15)));
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
12.0437 cycles

Running: maxperformance
28.5914 cycles
<<<
12.0437 / 28.5914 = 0.42123506
================================
#include <immintrin.h>
#include "common.h"

void max_performance(double*__restrict x, double*__restrict y, double*__restrict A)
{
    const __m256d sign1 = _mm256_setr_pd(-1.0, 1.0, -1.0, 1.0);
    const __m256d sign2 = _mm256_setr_pd(-1.0, 1.0, 1.0, -1.0);
    const __m256d sign3 = _mm256_setr_pd(-1.0, -1.0, 1.0, 1.0);

    for (int i = 0; i < 2; ++i) {
        for (int j = 0; j < 2; ++j) {
            __m256d yv = _mm256_loadu_pd(y + 4*j);

            __m256d a = _mm256_broadcast_sd(x+4*i);
            __m256d b = _mm256_broadcast_sd(x+4*i+1);
            __m256d c = _mm256_broadcast_sd(x+4*i+2);
            __m256d d = _mm256_broadcast_sd(x+4*i+3);

            __m256d am = _mm256_mul_pd(a, yv);
            __m256d bm = _mm256_mul_pd(b, yv);
            __m256d cm = _mm256_mul_pd(c, yv);
            __m256d dm = _mm256_mul_pd(d, yv);

            __m256d t1 = _mm256_mul_pd(_mm256_permute4x64_pd(bm, 0xB1), sign1);
            __m256d t2 = _mm256_mul_pd(_mm256_permute4x64_pd(cm, 0x4E), sign2);
            __m256d t3 = _mm256_mul_pd(_mm256_permute4x64_pd(dm, 0x1B), sign3);

            __m256d res = _mm256_add_pd(_mm256_add_pd(am, t1), _mm256_add_pd(t2, t3));
            _mm256_storeu_pd(A + 4*(i*2 + j), res);
        }
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
11.8367 cycles

Running: maxperformance
10.5185 cycles
<<<
11.8367 / 10.5185 = 1.12532205
================================
#include <immintrin.h>
#include "common.h"

inline double sum_avx(__m256d v) {
    __m256d hadd = _mm256_hadd_pd(v, v);
    __m256d perm = _mm256_permute2f128_pd(hadd, hadd, 0x01);
    __m256d sum = _mm256_add_pd(hadd, perm);
    return _mm256_cvtsd_f64(sum);
}

void max_performance(double* __restrict x, double* __restrict y, double* __restrict A) {
    const __m256d sign_r0 = _mm256_setr_pd(1.0, -1.0, -1.0, -1.0);
    const __m256d sign_r1 = _mm256_setr_pd(1.0, 1.0, 1.0, -1.0);
    const __m256d sign_r2 = _mm256_setr_pd(1.0, -1.0, 1.0, 1.0);
    const __m256d sign_r3 = _mm256_setr_pd(1.0, 1.0, -1.0, 1.0);

    __m256d x0 = _mm256_loadu_pd(x);
    __m256d x1 = _mm256_loadu_pd(x + 4);
    __m256d y0 = _mm256_loadu_pd(y);
    __m256d y1 = _mm256_loadu_pd(y + 4);

    // i=0, j=0
    __m256d p0 = _mm256_mul_pd(x0, sign_r0); __m256d m0 = _mm256_mul_pd(p0, y0);
    __m256d p1 = _mm256_mul_pd(x0, sign_r1); __m256d sw1 = _mm256_permute_pd(y0, 0x05); __m256d m1 = _mm256_mul_pd(p1, sw1);
    __m256d p2 = _mm256_mul_pd(x0, sign_r2); __m256d sw2 = _mm256_permute4x64_pd(y0, 0x4E); __m256d m2 = _mm256_mul_pd(p2, sw2);
    __m256d p3 = _mm256_mul_pd(x0, sign_r3); __m256d sw3 = _mm256_permute4x64_pd(y0, 0x1B); __m256d m3 = _mm256_mul_pd(p3, sw3);
    _mm256_storeu_pd(A, _mm256_setr_pd(sum_avx(m0), sum_avx(m1), sum_avx(m2), sum_avx(m3)));

    // i=0, j=1
    p0 = _mm256_mul_pd(x0, sign_r0); m0 = _mm256_mul_pd(p0, y1);
    p1 = _mm256_mul_pd(x0, sign_r1); sw1 = _mm256_permute_pd(y1, 0x05); m1 = _mm256_mul_pd(p1, sw1);
    p2 = _mm256_mul_pd(x0, sign_r2); sw2 = _mm256_permute4x64_pd(y1, 0x4E); m2 = _mm256_mul_pd(p2, sw2);
    p3 = _mm256_mul_pd(x0, sign_r3); sw3 = _mm256_permute4x64_pd(y1, 0x1B); m3 = _mm256_mul_pd(p3, sw3);
    _mm256_storeu_pd(A+4, _mm256_setr_pd(sum_avx(m0), sum_avx(m1), sum_avx(m2), sum_avx(m3)));

    // i=1, j=0
    p0 = _mm256_mul_pd(x1, sign_r0); m0 = _mm256_mul_pd(p0, y0);
    p1 = _mm256_mul_pd(x1, sign_r1); sw1 = _mm256_permute_pd(y0, 0x05); m1 = _mm256_mul_pd(p1, sw1);
    p2 = _mm256_mul_pd(x1, sign_r2); sw2 = _mm256_permute4x64_pd(y0, 0x4E); m2 = _mm256_mul_pd(p2, sw2);
    p3 = _mm256_mul_pd(x1, sign_r3); sw3 = _mm256_permute4x64_pd(y0, 0x1B); m3 = _mm256_mul_pd(p3, sw3);
    _mm256_storeu_pd(A+8, _mm256_setr_pd(sum_avx(m0), sum_avx(m1), sum_avx(m2), sum_avx(m3)));

    // i=1, j=1
    p0 = _mm256_mul_pd(x1, sign_r0); m0 = _mm256_mul_pd(p0, y1);
    p1 = _mm256_mul_pd(x1, sign_r1); sw1 = _mm256_permute_pd(y1, 0x05); m1 = _mm256_mul_pd(p1, sw1);
    p2 = _mm256_mul_pd(x1, sign_r2); sw2 = _mm256_permute4x64_pd(y1, 0x4E); m2 = _mm256_mul_pd(p2, sw2);
    p3 = _mm256_mul_pd(x1, sign_r3); sw3 = _mm256_permute4x64_pd(y1, 0x1B); m3 = _mm256_mul_pd(p3, sw3);
    _mm256_storeu_pd(A+12, _mm256_setr_pd(sum_avx(m0), sum_avx(m1), sum_avx(m2), sum_avx(m3)));
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
11.9042 cycles

Running: maxperformance
35.5114 cycles
<<<
11.9042 / 35.5114 = 0.33522193
================================