Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include "common.h"

void slow_performance(double*__restrict x, double*__restrict y, double*__restrict A) {
    for (int i = 0; i < 2; i++) {
        for (int j = 0; j < 2; j++) {
            double a = x[4 * i + 0];
            double b = x[4 * i + 1];
            double c = x[4 * i + 2];
            double d = x[4 * i + 3];

            double e = y[4 * j + 0];
            double f = y[4 * j + 1];
            double g = y[4 * j + 2];
            double h = y[4 * j + 3];

            // Quaternion multiplication:
            // (a + bi + cj + dk) * (e + fi + gj + hk) =
            // (ae - bf - cg - dh) +
            // (af + be + ch - dg)i +
            // (ag - bh + ce + df)j +
            // (ah + bg - cf + de)k
            double r0 = a * e - b * f - c * g - d * h;
            double r1 = a * f + b * e + c * h - d * g;
            double r2 = a * g - b * h + c * e + d * f;
            double r3 = a * h + b * g - c * f + d * e;

            int index = 4 * (i * 2 + j);
            A[index + 0] = r0;
            A[index + 1] = r1;
            A[index + 2] = r2;
            A[index + 3] = r3;
        }
    }
}

void max_performance(double*__restrict x, double*__restrict y, double*__restrict A)
{
}
```


================================
#include <immintrin.h>
#include "common.h"

void max_performance(double*__restrict x, double*__restrict y, double*__restrict A) {
    __m256d x0 = _mm256_loadu_pd(x);
    __m256d x1 = _mm256_loadu_pd(x + 4);
    __m256d y0 = _mm256_loadu_pd(y);
    __m256d y1 = _mm256_loadu_pd(y + 4);
    __m256d y2 = _mm256_loadu_pd(y + 8);
    __m256d y3 = _mm256_loadu_pd(y + 12);

    // Quaternion multiplication for (x0, y0)

    __m256d a = _mm256_permute4x64_pd(x0, 0b00000000); // aaaaaa
    __m256d b = _mm256_permute4x64_pd(x0, 0b01010101); // bbbbbb
    __m256d c = _mm256_permute4x64_pd(x0, 0b10101010); // cccccccc
    __m256d d = _mm256_permute4x64_pd(x0, 0b11111111); // dddddddd

    __m256d e = _mm256_permute4x64_pd(y0, 0b00000000); // eeeeeeee
    __m256d f = _mm256_permute4x64_pd(y0, 0b01010101); // ffffffff
    __m256d g = _mm256_permute4x64_pd(y0, 0b10101010); // gggggggg
    __m256d h = _mm256_permute4x64_pd(y0, 0b11111111); // hhhhhhhhh

    __m256d r0 = _mm256_sub_pd(_mm256_fmsub_pd(a, e, _mm256_fmsub_pd(b, f, _mm256_fmsub_pd(c, g, _mm256_mul_pd(d, h)))), _mm256_setzero_pd());
    __m256d r1 = _mm256_add_pd(_mm256_fmsub_pd(a, f, _mm256_mul_pd(d, g)), _mm256_fmsub_pd(b, e, _mm256_mul_pd(c, h)));
    __m256d r2 = _mm256_add_pd(_mm256_sub_pd(_mm256_mul_pd(a, g), _mm256_mul_pd(b, h)), _mm256_fmsub_pd(c, e, _mm256_mul_pd(d, f)));
    __m256d r3 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(a,h), _mm256_mul_pd(b,g)), _mm256_fmsub_pd(c,f, _mm256_mul_pd(d,e)));

    __m256d result0 = _mm256_unpacklo_pd(r0, r1);
    __m256d result1 = _mm256_unpackhi_pd(r0, r1);
    __m256d result2 = _mm256_unpacklo_pd(r2, r3);
    __m256d result3 = _mm256_unpackhi_pd(r2, r3);

    _mm256_storeu_pd(A, result0);
    _mm256_storeu_pd(A + 2, result2);
    _mm256_storeu_pd(A + 8, result1);
    _mm256_storeu_pd(A + 10, result3);

    // Quaternion multiplication for (x0, y1)

    e = _mm256_permute4x64_pd(y1, 0b00000000); // eeeeeeee
    f = _mm256_permute4x64_pd(y1, 0b01010101); // ffffffff
    g = _mm256_permute4x64_pd(y1, 0b10101010); // gggggggg
    h = _mm256_permute4x64_pd(y1, 0b11111111); // hhhhhhhhh

     r0 = _mm256_sub_pd(_mm256_fmsub_pd(a, e, _mm256_fmsub_pd(b, f, _mm256_fmsub_pd(c, g, _mm256_mul_pd(d, h)))), _mm256_setzero_pd());
     r1 = _mm256_add_pd(_mm256_fmsub_pd(a, f, _mm256_mul_pd(d, g)), _mm256_fmsub_pd(b, e, _mm256_mul_pd(c, h)));
     r2 = _mm256_add_pd(_mm256_sub_pd(_mm256_mul_pd(a, g), _mm256_mul_pd(b, h)), _mm256_fmsub_pd(c, e, _mm256_mul_pd(d, f)));
     r3 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(a,h), _mm256_mul_pd(b,g)), _mm256_fmsub_pd(c,f, _mm256_mul_pd(d,e)));

    result0 = _mm256_unpacklo_pd(r0, r1);
    result1 = _mm256_unpackhi_pd(r0, r1);
    result2 = _mm256_unpacklo_pd(r2, r3);
    result3 = _mm256_unpackhi_pd(r2, r3);


    _mm256_storeu_pd(A + 4, result0);
    _mm256_storeu_pd(A + 6, result2);
    _mm256_storeu_pd(A + 12, result1);
    _mm256_storeu_pd(A + 14, result3);

    // Quaternion multiplication for (x1, y0)

    a = _mm256_permute4x64_pd(x1, 0b00000000); // aaaaaa
    b = _mm256_permute4x64_pd(x1, 0b01010101); // bbbbbb
    c = _mm256_permute4x64_pd(x1, 0b10101010); // cccccccc
    d = _mm256_permute4x64_pd(x1, 0b11111111); // dddddddd

    e = _mm256_permute4x64_pd(y0, 0b00000000); // eeeeeeee
    f = _mm256_permute4x64_pd(y0, 0b01010101); // ffffffff
    g = _mm256_permute4x64_pd(y0, 0b10101010); // gggggggg
    h = _mm256_permute4x64_pd(y0, 0b11111111); // hhhhhhhhh

    r0 = _mm256_sub_pd(_mm256_fmsub_pd(a, e, _mm256_fmsub_pd(b, f, _mm256_fmsub_pd(c, g, _mm256_mul_pd(d, h)))), _mm256_setzero_pd());
    r1 = _mm256_add_pd(_mm256_fmsub_pd(a, f, _mm256_mul_pd(d, g)), _mm256_fmsub_pd(b, e, _mm256_mul_pd(c, h)));
    r2 = _mm256_add_pd(_mm256_sub_pd(_mm256_mul_pd(a, g), _mm256_mul_pd(b, h)), _mm256_fmsub_pd(c, e, _mm256_mul_pd(d, f)));
    r3 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(a,h), _mm256_mul_pd(b,g)), _mm256_fmsub_pd(c,f, _mm256_mul_pd(d,e)));

    result0 = _mm256_unpacklo_pd(r0, r1);
    result1 = _mm256_unpackhi_pd(r0, r1);
    result2 = _mm256_unpacklo_pd(r2, r3);
    result3 = _mm256_unpackhi_pd(r2, r3);

    _mm256_storeu_pd(A + 16, result0);
    _mm256_storeu_pd(A + 18, result2);
    _mm256_storeu_pd(A + 24, result1);
    _mm256_storeu_pd(A + 26, result3);

       // Quaternion multiplication for (x1, y1)

    e = _mm256_permute4x64_pd(y1, 0b00000000); // eeeeeeee
    f = _mm256_permute4x64_pd(y1, 0b01010101); // ffffffff
    g = _mm256_permute4x64_pd(y1, 0b10101010); // gggggggg
    h = _mm256_permute4x64_pd(y1, 0b11111111); // hhhhhhhhh

     r0 = _mm256_sub_pd(_mm256_fmsub_pd(a, e, _mm256_fmsub_pd(b, f, _mm256_fmsub_pd(c, g, _mm256_mul_pd(d, h)))), _mm256_setzero_pd());
     r1 = _mm256_add_pd(_mm256_fmsub_pd(a, f, _mm256_mul_pd(d, g)), _mm256_fmsub_pd(b, e, _mm256_mul_pd(c, h)));
     r2 = _mm256_add_pd(_mm256_sub_pd(_mm256_mul_pd(a, g), _mm256_mul_pd(b, h)), _mm256_fmsub_pd(c, e, _mm256_mul_pd(d, f)));
     r3 = _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(a,h), _mm256_mul_pd(b,g)), _mm256_fmsub_pd(c,f, _mm256_mul_pd(d,e)));

    result0 = _mm256_unpacklo_pd(r0, r1);
    result1 = _mm256_unpackhi_pd(r0, r1);
    result2 = _mm256_unpacklo_pd(r2, r3);
    result3 = _mm256_unpackhi_pd(r2, r3);


    _mm256_storeu_pd(A + 20, result0);
    _mm256_storeu_pd(A + 22, result2);
    _mm256_storeu_pd(A + 28, result1);
    _mm256_storeu_pd(A + 30, result3);
}
>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance
11.8598 cycles
Segmentation fault
<<<
WA
================================
#include <immintrin.h>
#include "common.h"

void max_performance(double*__restrict x, double*__restrict y, double*__restrict A) {
    __m256d x0 = _mm256_set_pd(x[0], x[1], x[2], x[3]);
    __m256d x1 = _mm256_set_pd(x[4], x[5], x[6], x[7]);

    __m256d y0 = _mm256_set_pd(y[0], y[1], y[2], y[3]);
    __m256d y1 = _mm256_set_pd(y[4], y[5], y[6], y[7]);

    __m256d a0 = _mm256_permute_pd(x0, 0b0000);
    __m256d b0 = _mm256_permute_pd(x0, 0b0101);
    __m256d c0 = _mm256_permute_pd(x0, 0b1010);
    __m256d d0 = _mm256_permute_pd(x0, 0b1111);

    __m256d a1 = _mm256_permute_pd(x1, 0b0000);
    __m256d b1 = _mm256_permute_pd(x1, 0b0101);
    __m256d c1 = _mm256_permute_pd(x1, 0b1010);
    __m256d d1 = _mm256_permute_pd(x1, 0b1111);

    // (ae - bf - cg - dh)
    __m256d r00 = _mm256_fmsubadd_pd(a0, y0, _mm256_mul_pd(_mm256_set_pd(b0[0], b0[1], b0[0], b0[1]), _mm256_set_pd(y0[0], y0[1], y1[0], y1[1])));
    r00 = _mm256_fmsubadd_pd(r00, _mm256_set_pd(c0[0], c0[1], c0[0], c0[1]), _mm256_mul_pd(_mm256_set_pd(y0[2], y0[3], y1[2], y1[3]), _mm256_set_pd(d0[0], d0[1], d0[0], d0[1])));
    // (af + be + ch - dg) i
    __m256d r10 = _mm256_fmaddsub_pd(a0, _mm256_set_pd(y0[1], y0[0], y1[1], y1[0]), _mm256_mul_pd(_mm256_set_pd(y0[2], y0[3], y1[2], y1[3]), _mm256_set_pd(d0[1], d0[0], d0[1], d0[0])));

    // (ag - bh + ce + df) j
    __m256d r20 = _mm256_fmaddsub_pd(x0, _mm256_set_pd(y0[2], y0[3], y1[2], y1[3]), _mm256_mul_pd(_mm256_set_pd(y0[1], y0[0], y1[1], y1[0]), _mm256_set_pd(b0[0], b0[1], b1[0], b1[1])));
    // (ah + bg - cf + de) k
    __m256d r30 = _mm256_fmaddsub_pd(x0, _mm256_set_pd(y0[3], y0[2], y1[3], y1[2]), _mm256_mul_pd(_mm256_set_pd(y0[1], y0[0], y1[1], y1[0]), _mm256_set_pd(c0[1], c0[0], c1[1], c1[0])));

    // Second row
    __m256d r01 = _mm256_fmsubadd_pd(a1, y0, _mm256_mul_pd(_mm256_set_pd(b1[0], b1[1], b1[0], b1[1]), _mm256_set_pd(y0[0], y0[1], y1[0], y1[1])));
    r01 = _mm256_fmsubadd_pd(r01, _mm256_set_pd(c1[0], c1[1], c1[0], c1[1]), _mm256_mul_pd(_mm256_set_pd(y0[2], y0[3], y1[2], y1[3]), _mm256_set_pd(d1[0], d1[1], d1[0], d1[1])));

    __m256d r11 = _mm256_fmaddsub_pd(a1, _mm256_set_pd(y0[1], y0[0], y1[1], y1[0]), _mm256_mul_pd(_mm256_set_pd(y0[2], y0[3], y1[2], y1[3]), _mm256_set_pd(d1[1], d1[0], d1[1], d1[0])));

    __m256d r21 = _mm256_fmaddsub_pd(x1, _mm256_set_pd(y0[2], y0[3], y1[2], y1[3]), _mm256_mul_pd(_mm256_set_pd(y0[1], y0[0], y1[1], y1[0]), _mm256_set_pd(b1[0], b1[1], b1[0], b1[1])));
    __m256d r31 = _mm256_fmaddsub_pd(x1, _mm256_set_pd(y0[3], y0[2], y1[3], y1[2]), _mm256_mul_pd(_mm256_set_pd(y0[1], y0[0], y1[1], y1[0]), _mm256_set_pd(c1[1], c1[0], c1[1], c1[0])));

    _mm256_storeu_pd(A + 0, r00);
    _mm256_storeu_pd(A + 4, r10);
    _mm256_storeu_pd(A + 8, r20);
    _mm256_storeu_pd(A + 12, r30);


    _mm256_storeu_pd(A + 16, r01);
    _mm256_storeu_pd(A + 20, r11);
    _mm256_storeu_pd(A + 24, r21);
    _mm256_storeu_pd(A + 28, r31);
}
>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance
12.1664 cycles
Segmentation fault
<<<
WA
================================
#include <immintrin.h>
#include "common.h"

void max_performance(double*__restrict x, double*__restrict y, double*__restrict A) {
    __m256d x0 = _mm256_loadu_pd(&x[0]);
    __m256d x1 = _mm256_loadu_pd(&x[4]);
    __m256d y0 = _mm256_loadu_pd(&y[0]);
    __m256d y1 = _mm256_loadu_pd(&y[4]);
    __m256d y2 = _mm256_loadu_pd(&y[8]);
    __m256d y3 = _mm256_loadu_pd(&y[12]);

    // Transpose x to have [a, b, c, d, a, b, c, d]
    __m256d x0_shuf = _mm256_permute4x64_pd(x0, _MM_SHUFFLE(3, 3, 1, 1));
    __m256d x1_shuf = _mm256_permute4x64_pd(x1, _MM_SHUFFLE(3, 3, 1, 1));
    x0_shuf = _mm256_blend_pd(x0_shuf, x1_shuf, 0b0011);
    __m256d x2_shuf = _mm256_permute4x64_pd(x0, _MM_SHUFFLE(2, 2, 0, 0));
    __m256d x3_shuf = _mm256_permute4x64_pd(x1, _MM_SHUFFLE(2, 2, 0, 0));
    x2_shuf = _mm256_blend_pd(x2_shuf, x3_shuf, 0b0011);

    // y0: [e, f, g, h, e, f, g, h]
    __m256d y0_shuf = _mm256_permute4x64_pd(y0, _MM_SHUFFLE(3, 3, 1, 1));
    __m256d y1_shuf = _mm256_permute4x64_pd(y1, _MM_SHUFFLE(3, 3, 1, 1));
    y0_shuf = _mm256_blend_pd(y0_shuf, y1_shuf, 0b0011);
    __m256d y2_shuf = _mm256_permute4x64_pd(y2, _MM_SHUFFLE(3, 3, 1, 1));
    __m256d y3_shuf = _mm256_permute4x64_pd(y3, _MM_SHUFFLE(3, 3, 1, 1));
    y2_shuf = _mm256_blend_pd(y2_shuf, y3_shuf, 0b0011);

    // r0 = ae - bf - cg - dh
    __m256d r0_0 = _mm256_mul_pd(_mm256_permute_pd(x0_shuf, 0b00), _mm256_permute_pd(y0_shuf, 0b00));
    __m256d r0_1 = _mm256_mul_pd(_mm256_permute_pd(x2_shuf, 0b00), _mm256_permute_pd(y2_shuf, 0b00));
    __m256d r0 = _mm256_sub_pd(r0_0, _mm256_add_pd(_mm256_mul_pd(_mm256_permute_pd(x0_shuf, 0b11), _mm256_permute_pd(y0_shuf, 0b11)),
                                              _mm256_mul_pd(_mm256_permute_pd(x2_shuf, 0b11), _mm256_permute_pd(y2_shuf, 0b11))));


    // r1 = af + be + ch - dg
    __m256d r1_0 = _mm256_mul_pd(_mm256_permute_pd(x0_shuf, 0b00), _mm256_permute_pd(y0_shuf, 0b11));
    __m256d r1_1 = _mm256_mul_pd(_mm256_permute_pd(x2_shuf, 0b00), _mm256_permute_pd(y2_shuf, 0b11));
    __m256d r1 = _mm256_add_pd(r1_0, _mm256_sub_pd(_mm256_mul_pd(_mm256_permute_pd(x0_shuf, 0b11), _mm256_permute_pd(y0_shuf, 0b00)),
                                              _mm256_mul_pd(_mm256_permute_pd(x2_shuf, 0b11), _mm256_permute_pd(y2_shuf, 0b00))));


    // Assemble final results
    __m256d r0_final = _mm256_permute4x64_pd(_mm256_shuffle_pd(r0, r1, 0b01), 0b00000000);
    __m256d r1_final = _mm256_permute4x64_pd(_mm256_shuffle_pd(r0, r1, 0b10), 0b00000000);
    __m256d r2_final = _mm256_permute4x64_pd(_mm256_shuffle_pd(r0, r1, 0b01), 0b11111111);
    __m256d r3_final = _mm256_permute4x64_pd(_mm256_shuffle_pd(r0, r1, 0b10), 0b11111111);

    _mm256_storeu_pd(&A[0], r0_final);
    _mm256_storeu_pd(&A[4], r1_final);
    _mm256_storeu_pd(&A[8], r2_final);
    _mm256_storeu_pd(&A[12], r3_final);
}
>>>
Starting program. 2 functions registered.
The result of the 2th function is not correct.

Running: slow_performance
12.0893 cycles

Running: maxperformance
18.127 cycles
<<<
WA
================================