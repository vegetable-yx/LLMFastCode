Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include "common.h"

void slow_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
    for (int j = 0; j < NC; j++)
        y[j] = 0;

    for (int i = 0; i < NR; i++)
    {
        for (int j = 0; j < NC; j++)
        {
            y[j] += A[i * NC + j] * x[i];
        }
    }
}

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
}
```


================================
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
    const int unroll_factor = 4;
    __m256d zero = _mm256_set1_pd(0.0);
    for (int j = 0; j < NC; j += unroll_factor) {
        __m256d y0 = zero;
        for (int i = 0; i < NR; i++) {
            __m256d A_vector = _mm256_loadu_pd(A + i * NC + j);
            __m256d x_vector = _mm256_set1_pd(x[i]);
            y0 = _mm256_fmadd_pd(A_vector, x_vector, y0);
        }
        _mm256_storeu_pd(y + j, y0);
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
213.079 cycles

Running: max_performance
648.035 cycles
<<<
213.079 / 648.035 = 0.32880786
================================
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
    const int NC_register = NC;
    const int NR_register = NR;

    // Initialize y to 0 using AVX2
    __m256d zero = _mm256_set1_pd(0.0);
    for (int j = 0; j < NC; j += 4)
    {
        _mm256_storeu_pd(&y[j], zero);
    }

    for (int i = 0; i < NR_register; ++i)
    {
        __m256d xi = _mm256_broadcast_sd(&x[i]);
        for (int j = 0; j < NC_register; j += 4)
        {
            __m256d Aj = _mm256_loadu_pd(&A[i * NC_register + j]);
            __m256d yi = _mm256_loadu_pd(&y[j]);
            yi = _mm256_fmadd_pd(Aj, xi, yi);
            _mm256_storeu_pd(&y[j], yi);
        }
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
211.613 cycles

Running: max_performance
250.373 cycles
<<<
211.613 / 250.373 = 0.84519098
================================
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y) {
    __m256d zero = _mm256_set1_pd(0.0);
    __m256d *y_simd = (__m256d *)y;
    for (int j = 0; j < NC / 4; j++) {
        y_simd[j] = zero;
    }
    for (int i = 0; i < NR; i++) {
        __m256d xi = _mm256_set1_pd(x[i]);
        for (int j = 0; j < NC / 4; j++) {
            __m256d Aj = _mm256_load_pd(A + i * NC + j * 4);
            y_simd[j] = _mm256_fmadd_pd(Aj, xi, y_simd[j]);
        }
    }
    for (int j = 0; j < (NC % 4); j++) {
        y[NC - 4 + j] += A[(NR - 1) * NC + NC - 4 + j] * x[NR - 1];
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
209.154 cycles

Running: max_performance
207.385 cycles
<<<
209.154 / 207.385 = 1.00853003
================================