Rewrite `slow_performance` into `max_performance`, keeping inputs, outputs, and behavior identical with allowable total squared precision error of 1e-3.  
Only apply performance optimizations (algebraic simplification, cache-friendly memory access, SIMD/AVX-2, loop unrolling, etc.).  

**Assume the code executes on a single core, DO NOT introduce multithreading or parallelism.**

**Target system**
- Intel(R) Core(TM) Ultra 7 265K CPU @ 3.90GHz
- Caches: 240 KB L1 D + 32 KB L1 I per core, 3.00 MB L2 per core, 30 MB shared L3  
- GCC 12 on Debian Bookworm; compile with `-O3 -march=native -mavx2 -std=c++11`

**Hints**
- Write SIMDe code with <immintrin.h> whenever possible.
- Do not use libc functions as their performance is not good.

Return **only** the full, ready-to-compile `max_performance` functionâ€”no other text, comments, or explanations.


```
#include <immintrin.h>
#include "common.h"

void slow_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
    for (int j = 0; j < NC; j++)
        y[j] = 0;

    for (int i = 0; i < NR; i++)
    {
        for (int j = 0; j < NC; j++)
        {
            y[j] += A[i * NC + j] * x[i];
        }
    }
}

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
}
```


================================
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
    for (int j = 0; j < NC; j++) {
        y[j] = 0;
    }

    const int unroll_factor = 8;
    const int aligned_NC = NC - (NC % unroll_factor);
    
    for (int i = 0; i < NR; i++) {
        __m256d xvec[unroll_factor/4];
        for (int k = 0; k < unroll_factor; k += 4) {
            xvec[k/4] = _mm256_broadcast_sd(x + i);
        }

        int j = 0;
        for (; j < aligned_NC; j += unroll_factor) {
            const int base = i * NC + j;
            __m256d a0 = _mm256_loadu_pd(&A[base]);
            __m256d a1 = _mm256_loadu_pd(&A[base + 4]);
            __m256d y0 = _mm256_loadu_pd(&y[j]);
            __m256d y1 = _mm256_loadu_pd(&y[j + 4]);

            y0 = _mm256_fmadd_pd(a0, xvec[0], y0);
            y1 = _mm256_fmadd_pd(a1, xvec[1], y1);

            _mm256_storeu_pd(&y[j], y0);
            _mm256_storeu_pd(&y[j + 4], y1);
        }

        for (; j < NC; j++) {
            y[j] += A[i * NC + j] * x[i];
        }
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
209.134 cycles

Running: max_performance
249.261 cycles
<<<
209.134 / 249.261 = 0.83901613
================================
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y) {
    // Zero y using vectorized instructions
    const int VECSIZE = 4;
    for (int j = 0; j <= NC - VECSIZE; j += VECSIZE) {
        _mm256_storeu_pd(&y[j], _mm256_setzero_pd());
    }
    for (int j = NC - (NC % VECSIZE); j < NC; ++j) {
        y[j] = 0.0;
    }

    // Process each row
    for (int i = 0; i < NR; ++i) {
        const double xi = x[i];
        if (xi == 0.0) continue; // Skip zero contributions

        // Load broadcast vector for xi
        const __m256d xvec = _mm256_set1_pd(xi);
        const double* A_row = &A[i * NC];
        
        // Accumulate row with optimized vectorization
        int j = 0;
        for (; j <= NC - VECSIZE; j += VECSIZE) {
            __m256d a = _mm256_loadu_pd(&A_row[j]);
            __m256d yval = _mm256_loadu_pd(&y[j]);
            yval = _mm256_fmadd_pd(a, xvec, yval);
            _mm256_storeu_pd(&y[j], yval);
        }

        // Handle the remaining elements in scalar context
        for (; j < NC; ++j) {
            y[j] += A_row[j] * xi;
        }
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
209.562 cycles

Running: max_performance
698.237 cycles
<<<
209.562 / 698.237 = 0.30013019
================================
#include <immintrin.h>
#include "common.h"

void max_performance(double *__restrict x, double *__restrict A, double *__restrict y)
{
    const int nc = NC;
    int j = 0;
    __m256d zero = _mm256_setzero_pd();

    // Vectorized zero initialization
    for (; j <= nc - 4; j += 4)
        _mm256_storeu_pd(&y[j], zero);
    
    // Remaining 0 to 3 elements
    for (; j < nc; ++j)
        y[j] = 0.0;

    // Process each row i of A
    for (int i = 0; i < NR; ++i) {
        double xi = x[i];
        __m256d x_vec = _mm256_set1_pd(xi);
        const int row_base = i * NC;

        int j = 0;
        // AVX2 loop (4 elements)
        for (; j <= nc - 4; j += 4) {
            __m256d a_vec = _mm256_loadu_pd(&A[row_base + j]);
            __m256d y_vec = _mm256_loadu_pd(&y[j]);
            y_vec = _mm256_fmadd_pd(a_vec, x_vec, y_vec);
            _mm256_storeu_pd(&y[j], y_vec);
        }

        // Scalar loop for the remaining elements
        for (; j < nc; ++j)
            y[j] += A[row_base + j] * xi;
    }
}
>>>
Starting program. 2 functions registered.

Running: slow_performance
210.005 cycles

Running: max_performance
207.986 cycles
<<<
210.005 / 207.986 = 1.00970738
================================